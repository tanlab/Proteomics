{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing of proteomics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This only covers the models that are fit using proteomics data only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.linear_model import MultiTaskLasso\n",
    "from sklearn.metrics import r2_score \n",
    "from sklearn.metrics import mean_squared_error \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.random_projection import SparseRandomProjection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data with outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_semiRaw_data():\n",
    "    p100 = pd.read_csv('Final Data/p100_with_smiles_but_no_metadata.csv')\n",
    "    jtvae_conv = pd.read_csv('Final Data/smiles_to_jtvae.csv')\n",
    "    full_data = p100.merge(jtvae_conv, how='left',on='canonical_smiles')\n",
    "    full_data.index = full_data['cid']\n",
    "    full_data.drop('cid',axis=1,inplace=True)\n",
    "read_semiRaw_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pd.read_csv('Final Data/janky_interlaced_full_data.csv',index_col='cid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outlier correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data=jank_outlier_correction(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def jank_outlier_correction(full_data):\n",
    "    for cell_id in list(dict.fromkeys(full_data['cell_id'])):                               #do it cell spesific\n",
    "        print(cell_id)\n",
    "        _ = full_data[full_data['cell_id']==cell_id]\n",
    "        for canon in list(dict.fromkeys(_['canonical_smiles'])) :          #only work with non-duplicate canonicals\n",
    "            tmp=_[_['canonical_smiles']==canon]\n",
    "            stds = []\n",
    "            for column in tmp.columns[3:-56]:                       #skip metadata and jtvae\n",
    "                stds.append(np.std(tmp[column]))                    #get the standart deviation of a drug effect on 1 target\n",
    "            _mean = np.mean(stds)\n",
    "            for std in range(len(stds)):\n",
    "                column = tmp.columns[3+std]\n",
    "                if stds[std] > _mean:                               #if there are standart deviations that deviate more than others\n",
    "                    _sub = []\n",
    "                    for val in tmp[column]:                         #replace it's max value with the average of others\n",
    "                        _sub = abs(val - tmp[column])\n",
    "                    max_index = tmp[column].index[np.where(np.amax(_sub) == _sub)[0][0]]\n",
    "                    full_data[column].loc[max_index] = np.mean(tmp[column].iloc[np.where(np.amax(_sub)!=_sub)])\n",
    "    return full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jank_outlier_correction(full_data).to_csv('Final Data/janky_interlaced_full_data.csv') #write the cleaned data back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_metadata(data):\n",
    "    return data.drop(['cell_id','pert_type','canonical_smiles'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_and_target(data):\n",
    "    only_jtvae = pd.DataFrame()\n",
    "    for column in data.columns:\n",
    "        if column.startswith('jtvae'):\n",
    "            only_jtvae = pd.concat((only_jtvae, data[column]), axis=1)\n",
    "    return only_jtvae, data.drop(only_jtvae.columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_folds(compound_list, splits=5,random_state=666):\n",
    "    compound_list = list(dict.fromkeys(compound_list))\n",
    "    kf = KFold(shuffle=True,n_splits=splits,random_state=random_state)\n",
    "    kf.get_n_splits(compound_list)\n",
    "\n",
    "    fold_comp = []\n",
    "    for train_index, test_index in kf.split(compound_list):\n",
    "         fold_comp.append([np.asarray(compound_list)[train_index], np.asarray(compound_list)[test_index]])\n",
    "    return fold_comp   #list that hold tuples of train and test compounds (in that order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_cols(train, test, target_reduction, tr_type, whiten=True):\n",
    "    if tr_type == 'PCA':\n",
    "        reducer = PCA(n_components=target_reduction, whiten=whiten).fit(train)\n",
    "    elif tr_type == 'FastICA':\n",
    "        reducer = FastICA(n_components=target_reduction, whiten=whiten).fit(train)\n",
    "    elif tr_type == 'RandomProjection':\n",
    "        reducer = SparseRandomProjection(n_components=target_reduction).fit(train)\n",
    "    return reducer.transform(train), reducer.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_data_of_fold_i_for_gene(fold, full_data, cell, target_reduction=None, normalize=False, tr_type='PCA', whiten=True, n_splits=5, random_state=666):\n",
    "    data = full_data[full_data['cell_id'] == cell]\n",
    "    folds = get_folds(data['canonical_smiles'], splits=5, random_state=666)\n",
    "    train = pd.DataFrame()\n",
    "    test = pd.DataFrame()\n",
    "    for canonical_smile in list(dict.fromkeys(data['canonical_smiles'])):\n",
    "        if canonical_smile in folds[fold][0]:\n",
    "            train = pd.concat((train,data[data['canonical_smiles']==canonical_smile]),axis=0)\n",
    "        elif canonical_smile in folds[fold][1]:\n",
    "            test = pd.concat((test,data[data['canonical_smiles']==canonical_smile]),axis=0)\n",
    "        else:\n",
    "            raise ValueError('The smiles doesn\\'t exist in the folds.')\n",
    "    \n",
    "    train = filter_metadata(train)\n",
    "    test = filter_metadata(test)\n",
    "\n",
    "    X_train, y_train = get_feature_and_target(train)\n",
    "    X_test, y_test = get_feature_and_target(test)\n",
    "    \n",
    "    if normalize:\n",
    "        scaler = MinMaxScaler((0,100)).fit(y_train)\n",
    "        y_train = scaler.transform(y_train)\n",
    "        y_test = scaler.transform(y_test)\n",
    "    if target_reduction is not None:\n",
    "        y_train, y_test = reduce_cols(y_train, y_test, target_reduction, tr_type)\n",
    "    \n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_dim, output_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=64, input_dim=input_dim))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(units=32))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(units=32))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(units=output_dim, activation='linear'))\n",
    "    model.compile(optimizer='adam', loss='mean_absolute_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = get_data_of_fold_i_for_gene(0, full_data, 'MCF7', target_reduction=1, tr_type='PCA')\n",
    "# y_train  = np.asanyarray(y_train).ravel()\n",
    "# y_test  = np.asanyarray(y_test).ravel()\n",
    "ll = lgb.LGBMRegressor(n_estimators=100, learning_rate=0.001, boosting_type='goss', reg_alpha=0.1, reg_lambda=0.)#, early_stopping_rounds=150, objective='regression')\n",
    "# rgb = MultiOutputRegressor(ll)\n",
    "rgb = build_model(X_train.shape[1], y_train.shape[1])\n",
    "rgb.fit(X_train, y_train)#,eval_set=(X_test,y_test))\n",
    "mean_absolute_error(y_test, rgb.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D\n",
    "from keras.optimizers import RMSprop\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import optuna\n",
    "import mlflow.keras\n",
    "import mlflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    X_train, X_test, y_train, y_test = get_data_of_fold_i_for_gene(0, full_data, cell, target_reduction=None, tr_type='FastICA')\n",
    "    n_inputs = X_train.shape[1]\n",
    "    n_outputs = y_train.shape[1]\n",
    "    \n",
    "    n_outputs = trial.suggest_int('target',1,n_outputs)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(trial.suggest_int('hidden_size',1,200), input_dim=n_inputs, activation='relu'))\n",
    "    model.add(Dropout(trial.suggest_uniform('dropout_1st',0.0,1.0)))\n",
    "    model.add(Dense(trial.suggest_int('hidden_size2',0,200), activation='relu'))\n",
    "    model.add(Dropout(trial.suggest_uniform('dropout_2nd',0.0,1.0)))\n",
    "    model.add(Dense(n_outputs))\n",
    "    model.add(Dropout(trial.suggest_uniform('dropout_3rd',0.0,1.0)))\n",
    "\n",
    "    lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
    "    model.compile(loss='mae', optimizer=RMSprop(lr=lr))\n",
    "    \n",
    "    avg = []\n",
    "    for i in range(5):\n",
    "        X_train, X_test, y_train, y_test = get_data_of_fold_i_for_gene(i, full_data, 'MCF7', target_reduction=n_outputs, tr_type='FastICA', normalize=True)\n",
    "        model.fit(X_train, y_train,epochs=trial.suggest_int('epochs',1,10))\n",
    "        avg.append(mean_absolute_error(model.predict(X_test),y_test))\n",
    "    return np.mean(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_exact_model(params):\n",
    "    X_train, X_test, y_train, y_test = get_data_of_fold_i_for_gene(0, full_data, cell, target_reduction=None, tr_type='FastICA')\n",
    "    n_inputs = X_train.shape[1]\n",
    "    n_outputs = y_train.shape[1]\n",
    "    \n",
    "    n_outputs = params['target']\n",
    "    model = Sequential()\n",
    "    model.add(Dense(params['hidden_size'], input_dim=n_inputs, activation='relu'))\n",
    "    model.add(Dropout(params['dropout_1st']))\n",
    "    model.add(Dense(params['hidden_size2'], activation='relu'))\n",
    "    model.add(Dropout(params['dropout_2nd']))\n",
    "    model.add(Dense(n_outputs))\n",
    "    model.add(Dropout(params['dropout_3rd']))\n",
    "\n",
    "    lr = params['lr']\n",
    "    model.compile(loss='mae', optimizer=RMSprop(lr=lr))\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = get_data_of_fold_i_for_gene(0, full_data, cell, target_reduction=n_outputs, tr_type='FastICA', normalize=True)\n",
    "    X = pd.concat((X_train,X_test),axis=0)\n",
    "    y = pd.concat((pd.DataFrame(y_train),pd.DataFrame(y_test)),axis=0)\n",
    "    model.fit(X, y,epochs=params['epochs'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for cell in ['A549', 'MCF7', 'PC3']:\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=250)\n",
    "    mlflow.set_tracking_uri('http://tanlab.punoqun.cyou:5000')\n",
    "    mlflow.set_experiment('P100 Experiments')\n",
    "    mlflow.start_run()\n",
    "    mlflow.set_tag('Author', 'Bahattin')\n",
    "    mlflow.log_param('Model', 'NN')\n",
    "    mlflow.log_param('cell_id', cell)\n",
    "    mlflow.log_param('n_fold', '5')\n",
    "    mlflow.log_param('random_state', '666')\n",
    "    mlflow.log_param('optimizer', 'adam')\n",
    "    mlflow.log_param('loss', 'mean_absolute_error')\n",
    "    mlflow.log_param('target_reduction', 'FastICA')\n",
    "    \n",
    "    mlflow.log_param('mae', str(study.best_value))\n",
    "    for param in study.best_params:\n",
    "        mlflow.log_param(param, str(study.best_params[param]))\n",
    "    build_exact_model(study.best_params)\n",
    "    mlflow.keras.log_model(build_exact_model(study.best_params,cell))\n",
    "#     ma.log_artifact(mlflow.get_artifact_uri())\n",
    "    model.save(cell)\n",
    "    mlflow.end_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
